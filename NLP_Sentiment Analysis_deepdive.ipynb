{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68b1a8a",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "A bag-of-words is an approach to transform text to numeric form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740bbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de3e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54125465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00829c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 1 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c943968",
   "metadata": {},
   "source": [
    "We have transformed the first sentence of Anna Karenina to an array counting the frequencies of each word. However, the output is not very readable, is it? We are still missing the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e801e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = \"E:/Education/NLP/IMDB Dataset.csv\"\n",
    "reviews = pd.read_csv(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a345d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b396786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  after  all  also  an  and  any  are  as  at  ...  well  were  what  \\\n",
      "0      1      1    1     0   1    6    0    2   4   0  ...     1     0     2   \n",
      "1      1      0    2     0   0    7    0    2   0   0  ...     3     0     0   \n",
      "2      0      0    0     0   0    4    0    1   0   1  ...     1     0     0   \n",
      "3      0      0    3     0   0    4    0    2   2   0  ...     1     0     0   \n",
      "4      2      0    2     0   0    5    0    1   1   0  ...     0     0     1   \n",
      "\n",
      "   when  which  who  will  with  would  you  \n",
      "0     0      1    2     0     5      1    3  \n",
      "1     0      1    0     0     3      0    1  \n",
      "2     1      0    0     0     2      0    0  \n",
      "3     1      1    0     0     3      0    2  \n",
      "4     0      1    0     0     1      0    0  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a7903",
   "metadata": {},
   "source": [
    "In the above examples,bagofwords does not maintain the order of words and hence the context gets lost. To maintain context we use ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7569014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=200, max_features=300, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(max_features=300,ngram_range=(1,2),max_df=200)\n",
    "vect.fit(reviews.review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9156f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b88703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fbb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 for  1930  2007  2nd  abc  agents  airplane  airport  alexander  altman  \\\n",
      "0       0     0     0    0    0       0         0        0          0       0   \n",
      "1       0     0     0    0    0       0         0        0          0       0   \n",
      "2       0     0     0    0    0       0         0        0          0       0   \n",
      "3       0     0     0    0    0       0         0        0          0       0   \n",
      "4       0     0     0    0    0       0         0        0          0       0   \n",
      "\n",
      "   ...  warrior  waters  wax  welles  werewolf  willis  woody  wrestling  ya  \\\n",
      "0  ...        0       0    0       0         0       0      0          0   0   \n",
      "1  ...        0       0    0       0         0       0      0          0   0   \n",
      "2  ...        0       0    0       0         0       0      2          0   0   \n",
      "3  ...        0       0    0       0         0       0      0          0   0   \n",
      "4  ...        0       0    0       0         0       0      0          0   0   \n",
      "\n",
      "   yellow  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "[5 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee3ed6",
   "metadata": {},
   "source": [
    "## Tokenize the string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a664d",
   "metadata": {},
   "source": [
    "Create a new feature for the length of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c90c1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b818f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f85cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment  n_words\n",
      "0      One of the other reviewers has mentioned that ...  positive      380\n",
      "1      A wonderful little production. <br /><br />The...  positive      201\n",
      "2      I thought this was a wonderful way to spend ti...  positive      205\n",
      "3      Basically there's a family where a little boy ...  negative      175\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive      283\n",
      "...                                                  ...       ...      ...\n",
      "49995  I thought this movie did a down right good job...  positive      241\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative      138\n",
      "49997  I am a Catholic taught in parochial elementary...  negative      271\n",
      "49998  I'm going to have to disagree with the previou...  negative      240\n",
      "49999  No one expects the Star Trek movies to be high...  negative      150\n",
      "\n",
      "[50000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793686a1",
   "metadata": {},
   "source": [
    "Building a feature for the language - to detect the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(reviews)):\n",
    "    languages.append(detect_langs(reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "reviews['language'] = languages\n",
    "\n",
    "print(reviews.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb87b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
